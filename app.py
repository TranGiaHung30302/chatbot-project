import os
import warnings
import streamlit as st
from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings
from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_community.llms import Ollama

# áº¨n cáº£nh bÃ¡o
def warn(*args, **kwargs):
    pass
warnings.warn = warn
warnings.filterwarnings('ignore')

st.set_page_config(
    page_title="CTU AI Assistant", 
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS Ä‘á»ƒ táº¡o giao diá»‡n Ä‘áº¹p hÆ¡n
st.markdown("""
    <style>
    /* Background gradient */
    .main {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    }
    
    /* Container chÃ­nh */
    .block-container {
        padding-top: 2rem;
        padding-bottom: 2rem;
        background: rgba(255, 255, 255, 0.95);
        border-radius: 20px;
        box-shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
        backdrop-filter: blur(10px);
    }
    
    /* Header */
    h1 {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        font-size: 3rem !important;
        font-weight: 800 !important;
        text-align: center;
        margin-bottom: 0.5rem !important;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
    }
    
    /* Subtitle */
    .subtitle {
        text-align: center;
        color: #4a5568;
        font-size: 1.1rem;
        margin-bottom: 2rem;
        padding: 1rem;
        background: linear-gradient(to right, #f7fafc, #edf2f7, #f7fafc);
        border-radius: 10px;
        border-left: 4px solid #667eea;
    }
    
    /* Chat messages */
    .stChatMessage {
        background: white;
        border-radius: 15px;
        padding: 1rem;
        margin: 0.5rem 0;
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08);
        transition: transform 0.2s;
    }
    
    .stChatMessage:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.12);
    }
    
    /* User message */
    [data-testid="stChatMessageContent"] {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border-radius: 15px;
        padding: 1rem;
    }
    
    /* Input box */
    .stChatInput {
        border-radius: 25px;
        border: 2px solid #667eea;
        padding: 0.75rem;
        font-size: 1rem;
    }
    
    .stChatInput:focus {
        border-color: #764ba2;
        box-shadow: 0 0 0 3px rgba(102, 126, 234, 0.1);
    }
    
    /* Sidebar */
    .css-1d391kg {
        background: linear-gradient(180deg, #667eea 0%, #764ba2 100%);
    }
    
    [data-testid="stSidebar"] {
        background: linear-gradient(180deg, #f7fafc 0%, #edf2f7 100%);
    }
    
    /* Buttons */
    .stButton>button {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border-radius: 20px;
        border: none;
        padding: 0.5rem 2rem;
        font-weight: 600;
        transition: all 0.3s;
    }
    
    .stButton>button:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 12px rgba(102, 126, 234, 0.4);
    }
    
    /* Typing indicator */
    @keyframes typing {
        0%, 100% { opacity: 0.3; }
        50% { opacity: 1; }
    }
    
    .typing-indicator {
        display: inline-flex;
        gap: 4px;
    }
    
    .typing-indicator span {
        width: 8px;
        height: 8px;
        background: #667eea;
        border-radius: 50%;
        animation: typing 1.4s infinite;
    }
    
    .typing-indicator span:nth-child(2) {
        animation-delay: 0.2s;
    }
    
    .typing-indicator span:nth-child(3) {
        animation-delay: 0.4s;
    }
    
    /* Info boxes */
    .info-box {
        background: linear-gradient(135deg, #667eea15 0%, #764ba215 100%);
        border-left: 4px solid #667eea;
        padding: 1rem;
        border-radius: 10px;
        margin: 1rem 0;
    }
    
    /* Stats */
    .stat-card {
        background: white;
        padding: 1rem;
        border-radius: 10px;
        text-align: center;
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08);
    }
    
    .stat-number {
        font-size: 2rem;
        font-weight: 700;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
    }
    
    .stat-label {
        color: #718096;
        font-size: 0.9rem;
        margin-top: 0.5rem;
    }
    </style>
""", unsafe_allow_html=True)

# Header vá»›i icon
st.markdown("<h1>ğŸ¤– CTU AI Assistant</h1>", unsafe_allow_html=True)
st.markdown("""
    <div class="subtitle">
        <strong>Xin chÃ o! TÃ´i lÃ  trá»£ lÃ½ AI thÃ´ng minh cá»§a Äáº¡i há»c Cáº§n ThÆ¡</strong><br>
        ğŸ’¡ Há»c vá»¥ | ğŸ“š ÄÄƒng kÃ½ há»c pháº§n | ğŸ“‹ Quy Ä‘á»‹nh sinh viÃªn | ğŸ’° Há»c bá»•ng & Há»c phÃ­<br>
        <em>HÃ£y Ä‘áº·t cÃ¢u há»i - TÃ´i luÃ´n sáºµn sÃ ng há»— trá»£ báº¡n 24/7!</em>
    </div>
""", unsafe_allow_html=True)

# LOAD DOCUMENTS & BUILD VECTORSTORE
directory = './data-rag/'
all_documents = []

if not os.path.exists(directory):
    st.error("ThÆ° má»¥c 'data-rag' chÆ°a tá»“n táº¡i.")
    st.stop()

# =====================================================================
# 1) LOAD TÃ€I LIá»†U
@st.cache_resource
def load_documents():
    print("=== Load tÃ i liá»‡u láº§n Ä‘áº§u ===")
    directory = "./data-rag/"
    all_docs = []

    for root, dirs, files in os.walk(directory):
        for filename in files:
            filepath = os.path.join(root, filename)

            if filename.endswith(".txt"):
                loader = TextLoader(filepath, encoding="utf-8")
            elif filename.endswith(".pdf"):
                loader = PyPDFLoader(filepath)
            elif filename.endswith(".docx"):
                loader = Docx2txtLoader(filepath)
            else:
                continue

            try:
                all_docs.extend(loader.load())
            except Exception as e:
                print("Lá»—i load:", filepath, e)

    print("Tá»•ng tÃ i liá»‡u:", len(all_docs))
    return all_docs


documents = load_documents()


# =====================================================================
# 2) CHUNKING
@st.cache_resource
def split_docs(_docs):
    print("=== Chia nhá» vÄƒn báº£n láº§n Ä‘áº§u ===")
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)
    texts = splitter.split_documents(_docs)
    print("Tá»•ng chunk:", len(texts))
    return texts


texts = split_docs(documents)


# =====================================================================
# 3) LOAD EMBEDDINGS
@st.cache_resource
def load_embeddings():
    print("=== Load model embedding láº§n Ä‘áº§u ===")
    return HuggingFaceEmbeddings(
        model_name="BAAI/bge-m3",
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True}
    )


embeddings = load_embeddings()


# =====================================================================
# 4) LOAD HOáº¶C Táº O CHROMA DB
@st.cache_resource
def load_vectorstore(_emb, _texts, _persist_dir="chroma_db"):

    db_exists = os.path.exists(_persist_dir) and len(os.listdir(_persist_dir)) > 0

    if not db_exists:
        print("=== ChÆ°a cÃ³ DB â†’ Táº¡o má»›i ... ===")
        vs = Chroma.from_documents(
            _texts,
            _emb,
            collection_name="my_collection",
            persist_directory=_persist_dir
        )
        print("âœ“ DB Ä‘Ã£ táº¡o xong")
        return vs

    print("=== DB Ä‘Ã£ tá»“n táº¡i â†’ Load DB ===")
    vs = Chroma(
        collection_name="my_collection",
        embedding_function=_emb,
        persist_directory=_persist_dir
    )
    print("âœ“ DB Ä‘Ã£ load")
    return vs


vectorstore = load_vectorstore(embeddings, texts)

# Sidebar vá»›i thÃ´ng tin (di chuyá»ƒn xuá»‘ng sau khi load xong documents vÃ  texts)
with st.sidebar:
    st.markdown("### ğŸ¯ ThÃ´ng tin há»‡ thá»‘ng")
    st.markdown("---")
    
    # Stats
    col1, col2 = st.columns(2)
    with col1:
        st.markdown(f"""
            <div class="stat-card">
                <div class="stat-number">{len(documents)}</div>
                <div class="stat-label">TÃ i liá»‡u</div>
            </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown(f"""
            <div class="stat-card">
                <div class="stat-number">{len(texts)}</div>
                <div class="stat-label">Äoáº¡n vÄƒn</div>
            </div>
        """, unsafe_allow_html=True)
    
    st.markdown("---")
    st.markdown("### âš™ï¸ CÃ i Ä‘áº·t")
    
    # Temperature slider
    if "temperature" not in st.session_state:
        st.session_state.temperature = 0.7
    
    temperature = st.slider(
        "ğŸŒ¡ï¸ Äá»™ sÃ¡ng táº¡o",
        min_value=0.0,
        max_value=1.0,
        value=st.session_state.temperature,
        step=0.1,
        help="GiÃ¡ trá»‹ cao hÆ¡n = cÃ¢u tráº£ lá»i sÃ¡ng táº¡o hÆ¡n"
    )
    st.session_state.temperature = temperature
    
    # Top K slider
    if "top_k" not in st.session_state:
        st.session_state.top_k = 10
    
    top_k = st.slider(
        "ğŸ“Š Sá»‘ tÃ i liá»‡u tham kháº£o",
        min_value=3,
        max_value=15,
        value=st.session_state.top_k,
        step=1,
        help="Sá»‘ lÆ°á»£ng tÃ i liá»‡u liÃªn quan Ä‘á»ƒ tÃ¬m kiáº¿m"
    )
    st.session_state.top_k = top_k
    
    st.markdown("---")
    st.markdown("### ğŸ’¬ Quáº£n lÃ½")
    
    if st.button("ğŸ—‘ï¸ XÃ³a lá»‹ch sá»­ chat", use_container_width=True):
        st.session_state.messages = [
            SystemMessage(content="Báº¡n lÃ  má»™t trá»£ lÃ½ AI thÃ¢n thiá»‡n, luÃ´n tráº£ lá»i hoÃ n toÃ n báº±ng tiáº¿ng Viá»‡t, diá»…n Ä‘áº¡t tá»± nhiÃªn vÃ  dá»… hiá»ƒu."),
        ]
        st.rerun()
    
    st.markdown("---")
    st.markdown("""
        <div class="info-box">
            <strong>ğŸ”’ Báº£o máº­t</strong><br>
            Dá»¯ liá»‡u cá»§a báº¡n Ä‘Æ°á»£c<br>xá»­ lÃ½ cá»¥c bá»™ vÃ  an toÃ n
        </div>
    """, unsafe_allow_html=True)
    
    st.markdown("""
        <div style='text-align: center; color: #718096; font-size: 0.8rem; margin-top: 2rem;'>
            <strong>CTU AI Assistant v1.0</strong><br>
            Powered by Llama 3.2 & Chroma DB
        </div>
    """, unsafe_allow_html=True)

# Sá»­ dá»¥ng top_k tá»« sidebar
if "top_k" not in st.session_state:
    st.session_state.top_k = 10

retriever = vectorstore.as_retriever(search_kwargs={"k": st.session_state.top_k})

# Sá»­ dá»¥ng temperature tá»« sidebar
if "temperature" not in st.session_state:
    st.session_state.temperature = 0.7

llm = Ollama(model="llama3.2:3b", temperature=st.session_state.temperature)
system_prompt = """
Báº¡n lÃ  trá»£ lÃ½ AI thÃ¢n thiá»‡n, tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, dá»… hiá»ƒu, dÃ nh cho sinh viÃªn Äáº¡i há»c Cáº§n ThÆ¡.

HÆ°á»›ng dáº«n tráº£ lá»i:
- Chá»‰ tráº£ lá»i dá»±a trÃªn tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p (context).  
- Náº¿u thÃ´ng tin khÃ´ng cÃ³ trong tÃ i liá»‡u, nÃ³i rÃµ ráº±ng báº¡n **khÃ´ng tÃ¬m tháº¥y thÃ´ng tin phÃ¹ há»£p**.  
- TrÃ¬nh bÃ y cÃ¢u tráº£ lá»i rÃµ rÃ ng, ngáº¯n gá»n, cÃ³ gáº¡ch Ä‘áº§u dÃ²ng hoáº·c sá»‘ thá»© tá»± náº¿u cáº§n
- KhÃ´ng thÃªm dá»¯ liá»‡u hoáº·c suy Ä‘oÃ¡n ngoÃ i context.  
- KhÃ´ng láº·p láº¡i lá»‹ch sá»­ há»™i thoáº¡i.  

Dá»¯ liá»‡u ná»n:
Lá»‹ch sá»­ há»™i thoáº¡i:
{history}

TÃ i liá»‡u liÃªn quan:
{context}

"""


qa_prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    ("human", "{input}")
])

if "messages" not in st.session_state:
    st.session_state.messages = [
        SystemMessage(content="Báº¡n lÃ  má»™t trá»£ lÃ½ AI thÃ¢n thiá»‡n, luÃ´n tráº£ lá»i hoÃ n toÃ n báº±ng tiáº¿ng Viá»‡t, diá»…n Ä‘áº¡t tá»± nhiÃªn vÃ  dá»… hiá»ƒu."),
    ]



for message in st.session_state.messages:
    if isinstance(message, HumanMessage):
        with st.chat_message("user", avatar="ğŸ‘¤"):
            st.markdown(message.content)
    elif isinstance(message, AIMessage):
        with st.chat_message("assistant", avatar="ğŸ¤–"):
            st.markdown(message.content)

prompt = st.chat_input("ğŸ’­ Nháº­p cÃ¢u há»i cá»§a báº¡n...")
if prompt:
    # Hiá»ƒn thá»‹ tin nháº¯n ngÆ°á»i dÃ¹ng vá»›i icon
    with st.chat_message("user", avatar="ğŸ‘¤"):
        st.markdown(prompt)
    st.session_state.messages.append(HumanMessage(content=prompt))

    print("=== Báº®T Äáº¦U TRUY Váº¤N ===")
    relevant_docs = retriever.invoke(prompt)
    print(f"Sá»‘ tÃ i liá»‡u tÃ¬m tháº¥y: {len(relevant_docs)}")

    if not relevant_docs:
        st.warning("âš ï¸ KhÃ´ng tÃ¬m tháº¥y tÃ i liá»‡u liÃªn quan")
        context_documents_str = ""
    else:
        for i, doc in enumerate(relevant_docs):
            print(f"- TÃ i liá»‡u {i+1}: {doc.metadata.get('source', 'KhÃ´ng cÃ³ tÃªn')} | {len(doc.page_content)} kÃ½ tá»±")
        context_documents_str = "\n\n".join(doc.page_content for doc in relevant_docs)

    # Láº¥y 3 lÆ°á»£t chat gáº§n nháº¥t Ä‘á»ƒ giá»¯ ngá»¯ cáº£nh
    history_text = ""
    for msg in st.session_state.messages[-3:]:
        role = "NgÆ°á»i dÃ¹ng" if isinstance(msg, HumanMessage) else "Trá»£ lÃ½"
        history_text += f"{role}: {msg.content}\n"

    # Táº¡o prompt cho LLM
    qa_prompt_local = qa_prompt.partial(
        history=history_text,
        context=context_documents_str
    )

    chat_placeholder = st.chat_message("assistant", avatar="ğŸ¤–")
    with chat_placeholder:
        message_placeholder = st.empty()
        # Typing indicator vá»›i animation
        message_placeholder.markdown("""
            <div style='padding: 1rem;'>
                <div class='typing-indicator'>
                    <span></span>
                    <span></span>
                    <span></span>
                </div>
                <span style='margin-left: 1rem; color: #667eea;'>Äang suy nghÄ©...</span>
            </div>
        """, unsafe_allow_html=True)

    # Gá»i mÃ´ hÃ¬nh LLM
    llm_chain = {"input": RunnablePassthrough()} | qa_prompt_local | llm

    result = llm_chain.invoke(prompt)

    # Hiá»ƒn thá»‹ káº¿t quáº£ vá»›i format Ä‘áº¹p
    message_placeholder.markdown(f"""
        <div style='line-height: 1.8;'>
            {result}
        </div>
    """, unsafe_allow_html=True)

    # LÆ°u tin nháº¯n tráº£ lá»i vÃ o lá»‹ch sá»­
    st.session_state.messages.append(AIMessage(content=result))
